# Lesson 7 欢迎来到深度学习的世界

欢迎来到《PyTorch深度学习实战》课程的第二周！之前，我们已经介绍了GPU硬件、进行了深度学习框架PyTorch的部署与安装，并且学习了深度学习基本数据结构Tensor（张量）与关键库autograd的用法。本周我们将正式进入神经网络以及其他深度学习关键概念的学习。**作为0基础课程，我们假设你对机器学习、深度学习、人工智能等概念并没有深入的了解。即便你已经学习过一些相关概念，我依然建议你学习本课程的第一部分，作知识参考与补充用**。在本周学习开始之前，请确保你已经安装可以正常运行的PyTorch，并对Tensor概念有了一定的认识。全部课程将会在Jupyter上运行。

## 一、深度学习、机器学习、人工智能与神经网络

这是一个人人都熟悉“人工智能”一词的时代。学习这门课程的你，应该早就听说过深度学习、机器学习、人工智能这些专业名词。许多人是在研究过这些概念，并且深思熟虑之后才确定了方向、开始进行学习，但也有许多人可能是因学业或事业要求，需要完成一些深度学习相关的任务，在明确了解深度学习等概念之前就开始了学习。如果你是前者，那你非常具有前瞻性，如果你是后者，那你非常幸运，因为你将学习的技能可能是本世纪最重要、最有商业价值的技能之一。无论你是出于什么目的进行学习，你都需要对相关的专业名词有深入的理解。在这一节，我就带你了解一下神经网络是怎么诞生的，以及机器学习、深度学习、人工智能这些概念，究竟彼此之间有什么关系。

你可以在许多地方找到这些概念的定义，但是这些定义恐怕很难让你产生对这些概念的进一步认识。在技术的领域里，这些概念之间是有着复杂的历史关系的，所有的一切，都是围绕着人类最初的**预测未来**的渴望展开。

人们总是希望能够提前得知未来会发生的事，如果我能知道明天会下雨，我就会带伞，如果我们知道那支股票明天会涨，我就会在今天买入——提前预测短时间内的未来，会给我们的生活带来巨大的便利。如果这种短时间预测能够规模化，就会产生巨大的商业价值——1930年代的数学家，乔治盖洛普就曾经做到过，他在没有计算机、没有数据库的情况下，靠人力手算、正确预测了《乱世佳人》电影的票房，准确率达到94%，直接就让好莱坞制片人都跪下来叫爸爸，求着他做电影数据运营。

实际上，这种短时预测很多时候都可以被实现。比如，常常在海上航行的船员，看到云的流动、感受风的方向，就知道会不会下雨。有经验的股票经纪人、银行经纪人，在深入研究某个公司的状况之后，就能够判断这个公司的股票会不会涨……你可能已经发现了，这种预测和判断，需要的是在**某个领域深入的“知识和经验”**。但是，经验和知识是无法在一夜之间获得的，在商业价值极高的领域（例如电影、金融）就更是如此。即便学习相同的知识，每个人能够达到的水平也是不一致的。如果从“培养精尖人才”开始考虑预测的规模化问题，那成本就高得离谱了。

所以，预测的规模化非常难以实现。为了解决这个问题，许多学科的尖端人才都在“预测未来”这个领域努力过。以乔治盖洛普为代表的数学家、统计学家们一马当先，先提出了“用数学模型代替人类进行学习”的概念。依靠概率计算、模式拟合（pattern fit）等方法、数学家们创造了许多数学模型（最典型的就是线性回归、贝叶斯模型等），这些模型能够**根据历史数据学习出某种规律，并依赖这些规律实现预测**。比如最典型的线性回归，就是假设输入与输出数据之间是直线关系，通过历史数据拟合出直线后，再给出新的输入数据，线性回归模型就可以根据已经得出的直线，推断出输出。这些模型的学习方式与人类并不相似，但是他们确实拥有“从过去学习，并用学到的东西去进行预测”的能力。后来，**数学家们将能够根据历史数据实现预测或得出某种结果的计算步骤或计算方法称之为算法。**

| 关键概念：算法                                               |
| ------------------------------------------------------------ |
| 原本用以定义在解决问题时，按照某种固定步骤一定可以得到问题的结果的处理过程。后在数据科学领域，用以指代能够根据历史数据实现预测或得出某种结果的计算步骤或数学方法。 |

当算法出现之后，计算机科学家们也发现了机会。算法是数学方法，即便实现了学习和预测的过程，还是需要人为进行计算，没有规模化的可能。而计算机天生拥有规模化的能力，只要编好特定的程序，计算机可以在很短的时间内给出大量的预测结果，时效性很强。因此，计算机与数学的结合可谓是历史的必然，用计算机实现数学家们提出的算法，可以瞬间提升”学习“和”预测“的效率。从这一瞬间开始，预测的规模化真正成为了可能。现在唯一的问题，就是数学家们提出的算法，如何才能进行更准确的预测了。

当计算机科学家与数学家都一门心思在搞数学，各种研究算法时，从全新角度提出全新思想、并且从此让人工智能走上历史舞台的人们是——哲学家与心理学家们！

哲学家相信，人类的思考是一种自然现象，与地球自转、春秋更替一样，大脑的思考也是具有规律和机制的。只要有机制，就可以模仿，只要有规律，就可以复现，模仿其结构，复现其规律，就很有可能模拟出它的功能，实现像人类一样的学习和预测。具脑神经科学家研究，人脑中具有300亿个神经元，这些神经元通过叫做”轴突“的结构链接在一起。在人类进行学习或思考时，电信号会在神经元上传入、传出，并逐渐被大脑所理解。

![image-20240311150037371](/deep-learning/Part4/image-20240311150037371.png)

1943年，在哲学、数学、心理学以及脑神经科学的相互碰撞下，**模拟人脑结构、复现人脑思考规律、以制造和人相似的智慧为目的的算法——人工神经网络（ANN）诞生了**。它用一个简单的图形模拟了人脑的学习方式——我们使用圆来表示神经元，使用线来模拟”轴突“，数据就是电信号。我们从神经元左侧输入数据，让神经元处理数据，并从右侧输出预测结果，看起来就和人类学习的方式非常相似。

![image-20240311150319561](/deep-learning/Part4/image-20240311150319561.png)

一时之间，模拟人类智力的研究领域人声鼎沸，相关研究遍地开花，好不热闹。

| 关键概念：人工神经网络（ANN）                                |
| ------------------------------------------------------------ |
| 模拟人脑结构、复现人脑思考规律、以制造和人相似的智慧为目的的机器学习算法。 |

二战结束后，计算机科学与算法发展迅速，研究者们普遍接受了使用计算机来规模化算法学习和预测的过程。1950年，“图灵测试”被定义，如果一台计算机能够与人类展开对话而没有被辨别出其机器身份，那么这台机器就具有智能。1956年，“人工智能”概念也被提出，指代模拟人类智慧的各项相关研究。同年，“机器学习”概念被定义，用以表示使用计算机实现算法，使得计算机能够学习数据、获得经验、并实现预测的过程，同时，许多经典的预测模型、包括神经网络，也都被统称为"机器学习算法"。也因此，神经网络算法的许多思想，都与传统机器学习算法是共通的。

| 关键概念：机器学习                                           |
| ------------------------------------------------------------ |
| 使用计算机实现算法，使得计算机能够学习数据、获得经验、并实现预测或得出结果的过程。 |

就在各个学科都迅猛发展、热闹非凡的时候，神经网络——它扑街了。

发生了什么呢？人们发现，神经网络的设计理念——模拟人类大脑——确实非常先进，但是神经网络算法本身有几个致命的弱点：

**预测效果差**

- 人工神经网络算法本身存在着许多缺陷和局限，这让它的效果在许多实际数据上都很糟糕（最初的神经网络不能实现非线性预测、还存在严重的过拟合问题）

**数据需求大**

- 神经网络需要大量的数据去喂养，在缺乏数据的情况下，它的效果常常不如其他机器学习算法

**计算时间长**

- 大量的数据需要大量的时间进行计算，而神经网络的计算机制本身也很复杂。计算时间过长不仅占用资源、不利于调优，还大大削弱了预测的时效性。

而最致命的弱点是：

**根本无法达到人类智力水平**

- 无论如何训练，神经网络都不能达到人类的智力水平，短时间内也看不到其他商业应用场景。投资者们的幻想破灭，政府和金融机构都撤出投资。

没有资本，一切都停止了转动。很快，无论是学术界还是工业界，都发现神经网络是一条走不通的道路。同时期，逻辑回归、KNN、决策树等算法被逐渐发明出来，机器学习中的其他算法开始繁荣发展，神经网络却无人问津。在此之后，使用神经网络算法的学术论文有很大可能无法被发表，相关的学术项目也无法拉到经费，这又进一步压榨了神经网络研究者们的生存空间。十年之后，其他机器学习算法的性能也逐渐遇到了瓶颈，虽然不像神经网络一样处处都是致命的缺陷，但经典机器学习算法的预测效果也没有达到能够实现商业价值的程度，整体来说预测效果依然不佳。整个人工智能及机器学习研究都进入到寒冬期，三十年间，人工智能成了冷门的研究领域。

寒冬一直持续到2000年。与1970年代不同，新世纪到来时，神经网络算法被大规模应用的各项条件已经成熟：

**神经网络算法有了长足的进步**

- 在寒冬期间，坚信神经网络潜力的科学家们做出了许多的努力来改良神经网络。他们一面提升神经网络的预测效果、一面减少神经网络的训练时间，发明了反向传播等至今都十分关键的技术，在算法层面让神经网络上了一个台阶。这些改良的技术，我们都会在后续的课程《优化神经网络》的部分学到。

**全球数据量激增**

- 互联网的诞生、计算机的推广让全球数据量激增，并且让巨量数据的储存和使用成为了可能。训练神经网络所需的数据量不再成为问题。

**芯片、云技术迅猛发展，人类获得了前所未有的巨大算力**

- 比起1970年代，2000年之后的计算资源价格大幅下跌，同样价格可以购买到的计算资源越来越强大。当计算资源变得越来越容易获取，神经网络的计算速度得到提升，能够使用的数据量也变得更大。

算法、数据与算力的进步，完全释放了神经网络的潜力。当数据量够大、神经网络够复杂时，神经网络的性能会远远超出其他机器学习算法，甚至超越人类。这为预测算法的商业化应用打下了基础。

<img src="/deep-learning/Part4/image-20240311151024117.png" alt="image-20240311151024117" style="zoom:67%;" />

而最为关键的是，大家发现，我们之前弄错技能点了！**在大部分的商业应用场景，我们根本不需要人类级别的智力！我们只需要一个能够将大规模任务完成得很好的机器。**比如如火车站、飞机场的人脸识别，我们根本不需要使用神经网络创造出一个检票员，我们只需要一块屏幕，以及能够识别此人与其身份证照片数据的工具就可以了。

2006年时，神经网络的资深研究者Hinton提议，为神经网络的相关研究取一个单独的名字。这个名字，听起来得比机器学习厉害、比机器学习更有希望，它必须代表了神经网络越大、越深、效果就会越好的特性，这个名字将会成为人工智能实现的基础，并且在21世纪剩下的几十年中、成为非常关键的技术。这个名字就是今天我们都很熟悉的**深度学习**。

| 关键概念：深度学习                   |
| ------------------------------------ |
| 以研究人工神经网络算法为核心的学科。 |

说到这里，相信你已经对神经网络的相关事实有了深刻的印象，并且对深度学习相关概念有了清楚的认识。从历史发展的脉络来看，人工智能是模拟人类智慧的各项相关研究，它包括了模拟人类学习和预测能力的机器学习算法。机器学习算法赋予计算机学习的能力，而神经网络是机器学习中最强大、效果最优的算法。深度学习，是专门研究神经网络的学科，因此它是机器学习的子集。现在，我们说到的人工智能技术，一般泛指深度学习与机器学习技术，人工智能也是深度学习与机器学习实际应用的表现。

<img src="/deep-learning/Part4/image-20240311151153856.png" alt="image-20240311151153856" style="zoom: 67%;" />

还有一些说法认为，机器学习与人工智能是交叉学科，即部分机器学习算法是为人工智能技术服务，而人工智能技术中也只有一部分是机器学习。现在，这种说法也被越来越多的人所接受，但无论如何，机器学习和深度学习对于人工智能的实现是有巨大影响的。

如今的商业环境中，规模化应用的算法基本都是深度学习，或机器学习与深度学习的结合。我们如果想在算法领域深入下去，就必须学习深度学习。从今天的情况来看，深度学习算法还有极大的潜力，随着算法、数据与算力三个方向的不断发展，神经网络具备完全替代部分技巧性工作的可能性。即便神经网络的发展在今天就停止，它也已经极大地改变了我们的生活方式——现在，许多自媒体平台都有使用以深度学习为基础的推荐系统，如抖音、快手，同时，美团对外卖骑手的统筹、淘宝京东的”猜你喜欢“栏目，也都是深度学习应用的实际场景。下一节，我们就来看看深度学习在2021年酷炫的研究成果吧。

## 二、深度学习前沿研究成果及酷炫应用展示

在这里，我给大家总结了一些2021年酷炫的AI应用。其中许多视频来自油管，需要科学上网，注意，科学上网后不能上B站，如果你需要看B站的视频，可以先关闭科学上网工具。如果你对英文不熟悉，油管播放器的右下角可以自动调出字幕，你可以选择中文字幕。顺便一说，作为深度学习技术集大成的Google的是视频平台，油管的字幕是语音捕捉后自动翻译的，无论是捕捉还是翻译过程，都由深度学习算法完成，你可以尽情地体验油管带来的感受。接下来，来好好体验一下2021年AI领域的前沿研究成果吧：

【古代人物图像复原】

AI还原欧洲宫廷画作与蒙娜丽莎：https://b23.tv/82iUvc

AI还原宋朝皇帝，原来可以这么帅：https://b23.tv/YIBFHQ

AI还原乾隆后妃，延禧宫略众生相：https://b23.tv/zJ0IBw

<img src="/deep-learning/Part4/image-20240311151401050.png" alt="image-20240311151401050" style="zoom:67%;" />

【4K影像上色复原】

1929年的北京：https://youtu.be/7jfceEnkTfk

1911年的纽约：https://youtu.be/hZ1OgQL9_Cw

1929年的上海：https://b23.tv/11O3rB

<img src="/deep-learning/Part4/image-20240311151445842.png" alt="image-20240311151445842" style="zoom:67%;" />

【风格迁移】

影片风格迁移：https://youtu.be/fcnjHmBcLNQ

根据输入的照片，实现比划顺序预测，一笔一划实现风格迁移：https://youtu.be/dzJStceOaQs

实时风格迁移，绘制的同时就能够动起来：https://youtu.be/UiEaWkf3r9A

<img src="/deep-learning/Part4/image-20240311151511134.png" alt="image-20240311151511134" style="zoom:67%;" />

【动作迁移】

川普跳起书记舞：https://youtu.be/Zkrcx3_DtCw

<img src="/deep-learning/Part4/image-20240311151533649.png" alt="image-20240311151533649" style="zoom:67%;" />

【物体识别】

这才是IPhone最强功能，视觉无障碍功能体验：https://b23.tv/mfwkzN

【AI作画】

通过你绘制的草稿，生成细致的人脸画像：https://youtu.be/T740TIvS0Gw

【机器翻译】

使用深度学习的google实时翻译：https://youtu.be/06olHmcJjS0

【AI动效】

特朗普和蒙娜丽莎深情合唱《Unravel》：https://b23.tv/XpnOQ8

【AI视频增强技术】

《你的名字》24帧补到60帧，极致丝滑：https://b23.tv/vSyMkM

【语音模型】

神经网络5s之内克隆你的声音：https://youtu.be/0sR1rU3gLzQ

【AI图像增强技术】

去除马赛克，低分辨率图像修复至高分辨率：https://youtu.be/cgakyOI9r8M

【语言模型】

会写高考作文的神经网络：https://b23.tv/boYOYr

神经网络学习玩贪吃蛇：https://youtu.be/zIkBYwdkuTk

还有许多深度学习的应用场景无法在视觉上表现出来，例如推荐系统、搜索引擎，许多语音模型等等。还有哪些你能够想到的深度学习应用呢？还有哪些你见过，觉得非常酷炫的应用呢？

## 三、机器学习中的基本概念

我们现在已经了解了神经网络是如何诞生的、了解了它与人工智能、机器学习、深度学习都有什么关系。接下来，我们需要了解机器学习中的许多通用概念，并将其推广到神经网络之中，来帮助我们理解神经网络算法的方方面面。注意，虽然许多概念在深度学习和机器学习中有同样的含义，但因应用场景不同，这些概念的相关知识也会有所区别。在这里，我们只会讲解与深度学习相关的部分。

### 1.样本、特征、标签

在使用机器学习或深度学习算法之前，我们往往被赋予了需要完成的任务。在传统机器学习中，我们的入门级任务一般是这样的：我希望算法为我判断，一朵花是什么类型的花。于是我们收集了以下的数据：

![image-20240311151945179](/deep-learning/Part4/image-20240311151945179.png)

在机器学习中，我们使用的数据一般都是m行n列的二维数据表。这些表格通常是规则储存的数字或者文字，通常表示为NumPy中的array或者Pandas中的DataFrame（熟悉Python的你应该能够看出来，上图是一个DataFrame）。

在这个数据表中，横向的每一行就是“样本”（samples），是我们收集到的一条条数据。比如说现在展现的这个数据集，每条样本就是一朵花。

每一行前的数字是样本的索引（Index），也就是每一朵花的编号，这个编号对于每一个样本而言是独一无二的。

数据表中的列，则表示每个样本的一些属性，在机器学习中我们称其为“特征”（features），也叫做“字段”或“维度”，注意，这里的维度与二维表的维度可不是相同的含义，前者指的是n的大小，后者指的是.shape后返回的数字的个数。

展示的最后的一列是“标签”（label），也叫做目标变量（target，或者target variable）。标签也是样本的一种属性，不过通常来说，它是我们希望算法进行预测、判断的问题的**正确答案**。比如现在的问题是“一朵花是什么类型的花？”，而这个数据表中的标签就是“是哪一种花”。

在经典机器学习中，因为数据表总是由一个个的特征组成，所以我们一般把数据表（不包括标签的部分）称之为**特征矩阵**，往往使用大写且加粗的$X$来表示，同样的，我们使用字母$x$表示每个特征。

在深度学习的世界中，我们接到的任务往往是这样的：辨别下面的图片中，是猫还是狗。

![image-20240311152109731](/deep-learning/Part4/image-20240311152109731.png)

在这种预测要求下，数据就不再是二维表了，而是语音、图像、视频这些非结构化数据。对于深度学习而言，我们使用的基本数据结构是张量Tensor，单一的图像数据就需要三维Tensor来进行表示，语音和视频数据维度则更高，所以我们的入门数据集的结构往往看起来是这样的：

![image-20240311152138674](/deep-learning/Part4/image-20240311152138674.png)

或者是这样的：

<img src="/deep-learning/Part4/image-20240311152153266.png" alt="image-20240311152153266" style="zoom:67%;" />

在第一个例子中，我们的Tensor结构为[32,1,28,28]，这里的32就是索引，代表图像上的32张图，其中每张图都需要一个三维张量来表示。而在第二个例子中，因为只有一张图，所以没有索引，其结构也只有三维。

在高维张量的操作中，我们一般不会再区分”行列“，而是认为每个索引对应的对象就是一个样本。比如结构[32,1,28,28]，实际上就是32个三维Tensor所组成的四维Tensor，这32个三维Tensor就是32张图片，也就是32个样本，而这些三维Tensor所包含的内容，也就是32个样本分别对应的特征。由于我们所使用的张量往往是高于两个维度的，因此我们不能称其为“矩阵”，因此对于深度学习，我们一般称特征所在的张量叫做特征张量。同样的，我们还是用大写且加粗的$X$来表示特征张量。

由于数据是非结构化的，所以标签也不会”位于数据的最后一列“之列的。在深度学习中，标签几乎100%是和数据集分开的，不过这些标签看起来都很普通，和机器学习中的标签差不多：

![image-20240311152443351](/deep-learning/Part4/image-20240311152443351.png)

### 2.分类与回归

标签是机器学习中非常重要的一个概念，不同的标签指向了不同的问题。机器学习可以解决现实中广泛存在的各种问题，但是应用最广泛的问题主要是两类：分类与回归。

<img src="/deep-learning/Part4/image-20240311152530444.png" alt="image-20240311152530444" style="zoom:67%;" />

当机器学习在判断“有无”、“是否”、“是A还是B还是C”时，它预测出的答案会是某种类别，这就是分类问题，比如刚才我们假设的，判断一朵花是哪一种花。当机器学习在预测“是多少”，“增长多少”这些问题时，它给出的答案就是某个具体的数字，这就是回归问题。最典型的，预测某支股票会不会涨，是分类问题，预测某支股票会涨/跌多大幅度，是回归问题。你可能注意到，分类与回归的区别就在于预测目标的不同，在具体数据中，也就表现为标签的不同。

在分类问题中，标签是类别，表现为离散型变量（Categorical），往往是整数，如0、1、2等。

![image-20240311152615316](/deep-learning/Part4/image-20240311152615316.png)

而在回归问题中，标签是具体数字，表现为连续型变量（Continuous），往往是连续的浮点数。

![image-20240311152630082](/deep-learning/Part4/image-20240311152630082.png)

在机器学习中，标签是会随算法的任务的变化而变化的——比如，判断是哪一种花，标签就是“是哪种花”这一列，假设我们想预测的是花瓣的长度，那标签就是“花瓣长“这一列了。所以在许多经典机器学习的应用案例中，第一步是根据业务问题”定义标签“。幸运的是，在深度学习中，这种情况非常非常少。但在实际应用时，我们也可能会遇见需要抉择标签的时候，知道”标签“是可以变化的，是人为规定的，这一点非常重要。

当然，除了分类与回归之外，算法还有许多应用场景，但在最初学习深度学习的时候，我们会接触到的几乎都是分类或回归任务。我们会在未来的课程中慢慢给大家讲解更多的应用。

### 3.有监督算法与无监督算法

既然标签是人为规定的，那是否可以选择没有标签的算法呢？当然可以。对于有标签的任务，我们称其为“**有监督学习**”，包括众人耳熟能详的众多算法：KNN，决策树，支持向量机，线性回归，逻辑回归等等，自然也包括大部分神经网络。对于没有标签的任务，被称为“**无监督学习**”，包括了聚类分析，协同过滤（推荐系统中的核心算法之一），以及变分自编码器等深度学习算法。

有监督学习非常符合人工智能诞生之初人们对于算法的要求：从我们已知的历史数据中进行学习，然后去预测我们渴望了解的东西。无论是从过去的股价预测未来的股价，还是从过去的天气预测未来的天气，“历史数据”都带有“正确答案”，也就是有标签。现代大部分应用广泛的算法都是有监督学习，这些算法能够将可观察到、可记录到、并且已知答案的输入数据，转变为需要逻辑分析才能够得出的有价值的输出。在之后的课程中，我们将广泛体会到有监督学习的力量。

无监督学习，相对的，由于没有标签，所以我们没有给算法提供任何的正确答案。大多数时候，我们只是告诉无监督算法说：好了，这些是数据，你去学学看，然后告诉我你得到了什么。在深度学习中，无监督算法在许多时候都被我们用来作为辅助算法，以增强有监督算法的学习效果。

在实际使用中，还存在着半监督学习算法、强化学习等更加深入的领域。在所有这些领域，都有着大量的算法供我们学习和使用，在后续的课程中我们也会提到其中常用和经典的部分。

### 4.如何判断我的模型是一个好模型？

现代机器学习算法大约有几十个，每年还有许多新的算法在被提出，从中选出效果优秀、符合需求的模型也是机器学习中的重要课题。为此，我们需要模型的评价机制。如何判断模型是一个好模型呢？在这里，我们提出模型的评估三角：

![image-20240311152905202](/deep-learning/Part4/image-20240311152905202.png)

- **模型预测效果**

在机器学习能够落地的场景，模型进行判断/预测的效果一定是我们追求的核心目标。在工业场景，如人脸识别中，如果模型效果不能达到几乎100%准确，那我们就无法使用算法代替人工检查，因为没有人可以承担算法判断失败之后的责任。因为相似的理由，深度学习在医疗领域的应用永远只能处于“辅助医务人员进行判断”的地位。在一些其他场景，如推荐系统，虽然模型的效果可能不需要达到近乎100%的准确，但优秀的推荐系统所带来的效应是非常强大的，而效果不够好的算法则是又昂贵又失败的代码罢了。

对于不同类型的算法，我们有不同的模型评估指标，我们依据这些评估指标来衡量模型的判断/预测效果。在之前的课程中，你应该已经接触了线性回归的评估指标之一：SSE，也就是真实值与预测值的差异的平方和。在之后的学习中，我们会展开来谈不同算法的评估指标。当课程结束时，你将会了解如何评估有监督、无监督、强化学习模型的效果。

- **运算速度**

能够同时处理大量数据、可以在超短时间内极速学习、可以实时进行预测，是机器学习的重要优势。如果机器学习的判断速度不能接近/超越人类，那规模化预测就没有了根基。如果算法的运算速度太慢，也不利于调优和实验。同时，运算缓慢的算法可能需要占用更多的计算和储存资源，对企业来说成本会变得更高。事实上，现代神经网络做出的许多改进，以及算法工程师岗位对于数据结构方面的知识要求，都是为了提升神经网络的运算速度而存在的的。**在模型效果不错的情况下保障运算速度较快**，是机器学习中重要的一环。

- **可解释性**

机器学习是一门技术，是一门有门槛的技术，曲高客寡，大众注定不太可能在3、5分钟之内就理解机器学习甚至深度学习算法的计算原理。但是技术人员肩负着要向老板，客户，同事，甚至亲朋好友解释机器学习在做什么的职责，否则算法的预测结果很可能不被利益相关人员所接受。尤其是在算法做出一些涉及到道德层面的判断时，可解释性就变得更加重要——例如前段时间闹得沸沸扬扬的UBER算法解雇UBER司机事件，UBER算法在司机们违规之前就预测他们会违约，因而解雇了他们，从算法的角度来看没什么问题，但由于UBER无法向司机们解释算法具体的运行规则，司机们自然也不会轻易接受自己被解雇的事实。**幸运的是，随着人工智能相关知识的普及，人们已经不太在意深度学习领域的可解释性了（因为神经网络在预测效果方面的优势已经全面压倒了它在其他方面的劣势）。**但在机器学习的其他领域，可解释性依然是非常关键的问题。

- **服务于业务**

只有服务于业务，或服务于推动人类认知的研究，算法才会具有商业价值。一个能100%预测你明天午饭内容的算法，或许对个人而言非常有用（解决了人生三大难题之一：午饭吃什么），但不会有公司为它投资，也不会有人希望将它规模化。机器学习算法的落地成本很高，因此企业会希望看到算法落地后确定的商业价值。只有资金流动，技术才能持续发展，算法才能继续发光发热。

在传统机器学习领域，评估三角的因素缺一不可，但在深度学习领域，没有什么比效果好、速度快更加重要。如果还能有一部分可解释性，那就是锦上添花了。当我们在训练深度学习模型时，我们会最优先考虑模型效果的优化，同时加快模型的运算速度。每时每刻，我们都是为了模型效果或预算速度而行动的。记住这一点，它会成为日后我们学习任何新知识时的动力。

## 四、实现深度学习：深入认识PyTorch框架

我们已经理解了神经网络是如何诞生的，也了解了怎样的算法才是一个优秀的算法，现在我们需要借助深度学习框架（Deep learning framework）来帮助我们实现神经网络算法。在本门课程中，我们所使用的深度学习框架是PyTorch，也就是Torch库的Python版本。之前我们已经仔细地介绍了PyTorch中的基本数据结构Tensor以及自动求梯度工具autograd，相信你已经对PyTorch有了一定的了解。在今天的课程中，我们将更深入地理解PyTorch库的构成以及PyTorch框架的设计理念，为之后熟练使用PyTorch打下基础。

| 加餐：框架是什么？框架（framework）与库（library）有什么区别？ |
| ------------------------------------------------------------ |
| 框架就是封装程度较高的库，PyTorch也可以被说成是用于构建深度学习算法的Python库。如果你熟悉Python，你应该很了解Pandas、NumPy这些工具库，我们用pandas和numpy来执行非常底层、非常基础的操作，所以一般我们不会认为他们是框架。但PyTorch、Sklearn都是在numpy等库的基础上再次进行了封装，形成的更高级别的库。在这些库中，我们基本不执行太底层和基础的操作，而是专注于构筑复杂的程序来解决问题。对于这样的库，我们称其为“框架”。从现在的角度来看，sklearn是比PyTorch更高层的框架，因为sklearn的封装比PyTorch更加严密。 |

### 1.PyTorch的优势

我们在上一节中说过，一个好的深度学习模型，应该是**预测结果优秀、计算速度超快、并且能够服务于业务（即实际生产环境）的**。巧合的是，PyTorch框架正是基于这样目标建立的。

一个神经网络算法的结果如何才能优秀呢？如果在机器学习中，我们是通过模型选择、调整参数、特征工程等事项来提升算法的效果，那在神经网络中，我们能够做的其实只有两件事：

**1）加大数据规模**

**2）调整神经网络的架构**，也就是调整网络上的神经元个数、网络的层数、信息在网络之间传递的方式。

对于第一点，PyTorch的优势是毫无疑问的。PyTorch由Facebook AI研究实验室研发。在2019年，FB每天都需要支持400万亿次深度学习算法预测，并且这个数据还在持续上涨，因此PyTorch天生被设计成非常适合进行巨量数据运算，并且可以支持分布式计算、还可以无缝衔接到NVIDIA的GPU上来运行。除此之外，PyTorch的运行方式有意被设计成更快速、更稳定的方式，这使得它运行效率非常高、速度很快，这一点你在使用PyTorch进行编程的时候就能够感受到。**为高速运行巨量数据下的神经网络而生，PyTorch即保证了神经网络结果优秀，又权衡了计算速度**。

而第二点，则是深度学习整个学科的灵魂操作。我们耳熟能详的RNN、CNN、LSTM等算法，其实都是在原始神经网络上进行了神经元、链接、或信息传递方式的改变而诞生的。因此，**一个优秀的深度学习框架，必须具备非常高的灵活性和可调试性，才有可能不断推进深度学习算法的研究**。（同时，一个优秀的深度学习算法工程师，必须具备灵活调用任何可用网络结构的能力，才可能搭建出适应实际工业场景的神经网络）。幸运的是，PyTorch框架的创始团队在建立PyTorch时的目标，就是建立**最灵活的框架来表达深度学习算法**，这为PyTorch能够最大程度释放神经网络算法的潜力、放大深度学习的本质优势、实现更好的算法效果提供了基础。

同时，为了能够让算法调试变得更加容易，PyTorch在设计之初就**支持****eager model**（类似于在jupyter notebook的运行方式，可以每写几行代码就运行，并且返回相应的结果，通常在研究原型时使用），而tensorflow等框架在最初是不支持eager model，只支持graph-based model（一次性写完全部的代码，编译后上传服务器进行全部运行，这种类型的代码更加适合部署到生产环境中）。现在的PyTorch使用JIT编辑器，使得代码能够在eager和graph model之间自由转换，tensorflow也在1.7版本之后补充了这个功能。

最难得的是，PyTorchAPI简化程度很高，代码确实简单易懂。在编程的世界里，封装越底层就越灵活（如C++就比Python更加灵活），但越底层的框架往往就越复杂，需要的代码量也越多。PyTorch建设团队在构筑PyTorch项目时，**一直遵守”简单胜于复杂“的原则**。为了让PyTorch尽量简单，他们参考了大量NumPy以及Python的基本语法，让PyTorch可以无缝衔接到Python中，在保留灵活性的同时，最高程度地简化了API，继承了Python的大部分语法风格。对于熟练使用Python的人来说，使用PyTorch库通常轻而易举。

除此之外，PyTorch十分重视从研发算法到工业应用的过程。在PyTorch的官网头图上，甚至能够直接看到from research to production（从研发到生产）的字样。他们甚至完整地定义了算法部署到实际环境中需要达到的数个要求，并在2019年QCon会议的演讲中详细地说明了PyTorch团队是如何围绕”从研发到生产“这个目标设计了PyTorch的部署模块JIT。详细的内容可以参考这里：https://youtu.be/EkELQw9tdWE（需科学上网）。

<img src="/deep-learning/Part4/image-20240311154307854.png" alt="image-20240311154307854" style="zoom:67%;" />

总结一下，PyTorch的优势可以概括为以下几点：

- 天生支持巨量数据和巨大神经网络的高速运算
- 灵活性高，足以释放神经网络的潜力，并且在保留灵活性的同时，又有Python语法简单易学的优势
- 支持研究环境与生产环境无缝切换，调试成本很低

作为深度学习框架，PyTorch可以说具备了几乎所有产出优质深度学习算法的条件。现在，PyTorch社区及围绕PyTorch的生态还在建设中，这可以说是它唯一的弱势了。作为一门深度学习入门课程，我们非常推荐你选择PyTorch作为你的第一个深度学习框架。但它是否会成为你的最后一个深度学习框架，将由你来决定。

### 2.PyTorch库的基本架构

虽然PyTorch库在建设时拥有许多先进的理念，但在库的规划整理这一点上，它却与大部分编程库一样，显得有些潦草。任何编程库都不是在一个完美的企划下被设计出来的，而是在实际应用中不断被探索出来的，这就导致大部分编程库的体系是混乱的——即初学者完全不知道应该从哪里开始学习，典型的代表就是matplotlib，NumPy这些明明很有用，但是官网写得不知所云的库。

PyTorch官网在深度学习领域常常受到赞扬，许多人认为PyTorch官网写得简单明了，容易上手。但如果是学过sklearn课堂的小伙伴，就会知道sklearn官网是多么规范、多么易学：

<img src="/deep-learning/Part4/image-20240311154509934.png" alt="image-20240311154509934" style="zoom:67%;" />

再看看PyTorch官网这令人窒息的，按字母表顺序排列的类的列表：

<img src="/deep-learning/Part4/image-20240311154528940.png" alt="image-20240311154528940" style="zoom:67%;" />

文档写得好？其实全是同行的衬托，反正比tf写得好一点点。

在开始学习PyTorch之前，我们对PyTorch的核心模块进行了梳理。现在PyTorch中的模块主要分为两大类：原生Torch库下，用于构建灵活神经网络的模块，以及成熟AI领域中，用以辅助具体行业应用的模块。

<img src="/deep-learning/Part4/image-20240311154600347.png" alt="image-20240311154600347" style="zoom:67%;" />

<img src="/deep-learning/Part4/image-20240311154604853.png" alt="image-20240311154604853" style="zoom:67%;" />

两大模块的层次是并列的，当我们导入库的时候，我们是这样做：

```python
import torch
import torchvision
```

实际上在我们对PyTorch进行安装的时候，我们也是同时安装了torch和torchvision等模块。

当我们需要优化算法时，我们运行的是：

```python
from torch import optim
```

看出库的层次区别了吗？在我们课程的前几周，我们会集中在Torch模块中，帮助大家熟悉PyTorch的基本操作，并培养从0建立起自定义神经网络的能力。在课程后续的篇章中，我们将会涉入成熟AI领域的许多库中，对成熟算法和先进的神经网络架构进行讲解。为了构建强大的神经网络，我们会交叉使用两个模块的内容。在我们学习的过程中，我们或许会用到不在这两张架构图上的库，后续我们会继续补充和修缮这两张架构图。

了解了这么多内容后，你终于可以开始学习神经网络了。PyTorch代码虽然简单，但运行一行简单的代码却需要丰富的基础知识。从下一节开始，我们从0学习神经网络，并逐渐让你掌握PyTorch中的代码。